{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = [1, 2,3] # List of GPU IDs to use\n",
    "output_npy_dir = '/p/vast1/lazin1/UNet_npy_output_feature_importance'\n",
    "\n",
    "if not os.path.exists(output_npy_dir):\n",
    "    os.makedirs(output_npy_dir)\n",
    "# # Load the pre-trained U-Net model\n",
    "# model = UNet(in_channels=6, out_channels=1)  # Adjust input/output channels as needed\n",
    "# model.load_state_dict(torch.load(\"unet_model.pt\"))  # Load your trained model\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# # Define the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Define the dataset and data loader\n",
    "# # Replace input_data and target_data with actual data tensors\n",
    "# input_data = torch.randn(4275, 6, 512, 512)  # 4275 samples, 6 features, 512x512 images\n",
    "# target_data = torch.randint(0, 2, (4275, 1, 512, 512), dtype=torch.float32)\n",
    "# dataset = TensorDataset(input_data, target_data)\n",
    "# data_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=5, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        features = init_features\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, features)\n",
    "        self.encoder2 = conv_block(features, features * 2)\n",
    "        self.encoder3 = conv_block(features * 2, features * 4)\n",
    "        self.encoder4 = conv_block(features * 4, features * 8)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.middle = conv_block(features * 8, features * 16)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = conv_block(features * 16, features * 8)\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(features * 8, features * 4)\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(features * 4, features * 2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(features * 2, features)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "\n",
    "        mid = self.middle(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(mid)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.out_conv(dec1)\n",
    "\n",
    "def test_model(model, test_loader, device='cuda', threshold=0.5):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    batch_outputs = []\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Apply sigmoid if using BCEWithLogitsLoss (for raw logits)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            # Set a threshold to convert probabilities to binary\n",
    "            threshold = 0.5\n",
    "            binary_predictions = (outputs > threshold).float()  # Now we have 0s and 1s\n",
    "            iou_score = mean_iou(outputs, targets, threshold)\n",
    "            iou_scores.append(iou_score)\n",
    "            batch_outputs.append(binary_predictions.cpu())\n",
    "    batch_outputs = torch.cat(batch_outputs, dim=0)\n",
    "    average_iou = sum(iou_scores) / len(iou_scores)\n",
    "    print(f\"Mean IoU on test data: {average_iou:.4f}\")\n",
    "    return batch_outputs,average_iou\n",
    "\n",
    "def load_model(model_path, device='cuda'):\n",
    "    model = UNet(in_channels=5, out_channels=1, init_features=32)\n",
    "    model = nn.DataParallel(model).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_dict(file_path):\n",
    "    \"\"\"\n",
    "    Loads a dictionary from a file using pickle.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file where the dictionary is saved.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The loaded dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        dictionary = pickle.load(f)\n",
    "    print(f\"Dictionary loaded from {file_path}\")\n",
    "    return dictionary\n",
    "\n",
    "# mIoU metric function\n",
    "def mean_iou(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "    union = (pred + target).clamp(0, 1).sum(dim=(1, 2, 3))\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return iou.mean().item()\n",
    "\n",
    "\n",
    "def test_image_output_array_event_wise(model, path_input_dict, path_target_dict, event_file, fold=int,  batch_size=64, device='cuda',EVENT_STR=str, device_ids=device_ids):\n",
    "\n",
    "        # Test the model on the validation set\n",
    "        output_array, average_iou = test_model(model, test_loader=data_loader, device=device)\n",
    "        print(f\"Output array shape for fold {fold }: {output_array.shape}\")\n",
    "        print(f\"Average IoU for fold {fold }: {average_iou}\")\n",
    "\n",
    "        # output_arrays.append(output_array)\n",
    "        # ious.append(average_iou)\n",
    "        output_npy = f\"{output_npy_dir}/{EVENT_STR}/{event_str}_event_wise.npz\"\n",
    "        \n",
    "        np.savez(output_npy, output_array, average_iou)\n",
    "        print(f\"{event_str}.npz saved\")\n",
    "\n",
    "\n",
    "\n",
    "# # Test with all features\n",
    "# def test_model(loader, model, device):\n",
    "#     model.eval()\n",
    "#     total_iou = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             total_iou += mean_iou(outputs, targets)\n",
    "#     return total_iou / len(loader)\n",
    "\n",
    "# Baseline performance with all features\n",
    "print(\"Evaluating baseline performance with all features...\")\n",
    "baseline_iou = test_model(data_loader, model, device)\n",
    "print(f\"Baseline mIoU: {baseline_iou:.4f}\")\n",
    "\n",
    "EVENT_STRS = [\"Mississippi_20190617_3AC6_non_flood\"]\n",
    "for e, EVENT_STR in enumerate(EVENT_STRS):\n",
    "    event_file = f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/EVENT_{EVENT_STR}.csv\" \n",
    "    events = pd.read_csv(event_file, header=None).to_numpy()\n",
    "    model_dir = f\"/p/vast1/lazin1/UNet_trains/{EVENT_STR}_event_wise\"\n",
    "    model_path = os.path.join(model_dir, f\"unet_model_fold_{len(events)}.pt\")\n",
    "    print(model_path)\n",
    "    model = load_model(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    path_input_dict = f\"/p/vast1/lazin1/UNet_inputs/{EVENT_STR}_input_dict.pkl\"\n",
    "    path_target_dict = f\"/p/vast1/lazin1/UNet_inputs/{EVENT_STR}_target_dict.pkl\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_dict = load_dict(path_input_dict)\n",
    "    target_dict = load_dict(path_target_dict)\n",
    "    events = pd.read_csv(event_file, header=None).to_numpy()\n",
    "\n",
    "    for event in events:\n",
    "        event_str= event[0].split(\"/\")[-1][:-4]\n",
    "        test_keys= [key for key in input_dict if key.startswith(event_str)]\n",
    "\n",
    "        # os.makedirs(f\"{output_npy_dir}/{EVENT_STR}\", exist_ok=True)\n",
    "\n",
    "        test_input = np.empty((len(test_keys),5,512,512),dtype=np.float32)\n",
    "        test_target = np.empty((len(test_keys),1,512,512),dtype=np.float32)\n",
    "\n",
    "\n",
    "        with open(f\"{output_npy_dir}/{EVENT_STR}/{event_str}.csv\", 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for t, test_key in enumerate(test_keys):\n",
    "                test_input[t] = input_dict[test_key]\n",
    "                test_target[t] = target_dict[test_key]\n",
    "                writer.writerow([test_key])\n",
    "\n",
    "        \n",
    "        \n",
    "        test_input_array = torch.from_numpy(np.array(test_input))\n",
    "        test_target_array = torch.from_numpy(np.array(test_target))\n",
    "        print(test_input.shape[0], test_target.shape[0])\n",
    "        \n",
    "        test_dataset = TensorDataset(test_input_array, test_target_array)\n",
    "        \n",
    "        \n",
    "        \n",
    "        data_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        output_array, baseline_average_iou = test_model(model, test_loader=data_loader, device=device)\n",
    "        # test_image_output_array_event_wise(model, path_input_dict, path_target_dict, event_file,batch_size=64, device='cuda',EVENT_STR=EVENT_STR, device_ids=device_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Perform LOFO analysis\n",
    "        feature_importances = []\n",
    "        for feature in range(test_input_array.shape[1]):\n",
    "            print(f\"\\nEvaluating model performance without feature {feature + 1}...\")\n",
    "\n",
    "            # Create a copy of the input data and set the current feature to zero\n",
    "            modified_data = test_input_array.clone()\n",
    "            modified_data[:, feature, :, :] = 0\n",
    "\n",
    "            # Create a new data loader with modified data\n",
    "            modified_dataset = TensorDataset(modified_data, test_target_array)\n",
    "            modified_loader = DataLoader(modified_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "            # Test model with the modified dataset\n",
    "            lofo_iou = test_model(modified_loader, model, device)\n",
    "            print(f\"mIoU without feature {feature + 1}: {lofo_iou:.4f}\")\n",
    "\n",
    "            # Calculate importance as the drop in mIoU\n",
    "            importance = baseline_iou - lofo_iou\n",
    "            feature_importances.append(importance)\n",
    "\n",
    "        # Display feature importances\n",
    "        print(\"\\nFeature importances (based on mIoU drop):\")\n",
    "        for i, imp in enumerate(feature_importances):\n",
    "            print(f\"Feature {i + 1}: Importance = {imp:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPID",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
