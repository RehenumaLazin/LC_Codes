{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as '/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_event1.hyg'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing CSV files\n",
    "event = \"event1\"\n",
    "# Define the date range\n",
    "start_date = pd.Timestamp(\"2022-12-29\")\n",
    "end_date = pd.Timestamp(\"2023-01-20\")\n",
    "\n",
    "\n",
    "# Generate a date range with a 24-hour frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='24h')\n",
    "\n",
    "# Calculate the number of hours since the start date and convert to integers\n",
    "hours_since_start = ((date_range - start_date).total_seconds() / 3600).astype(int)  # Convert seconds to hours and then to integers\n",
    "\n",
    "\n",
    "csv_dir = f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/Obs_Flow/{event}\"  # Change this to your CSV folder\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))  # Find all CSV files\n",
    "obs_location= f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/sonoma_USGS_Obs.csv\"\n",
    "df_location = pd.read_csv(obs_location)\n",
    "lats = df_location[\"LAT_GAGE\"]\n",
    "longs = df_location[\"LNG_GAGE\"]\n",
    "STAID = df_location[\"STAID\"].values.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Define column names (since there are no headers in files)\n",
    "columns = [\"Gauge ID\", \"Year\", \"Month\", \"Day\", \"Flow Data\"]\n",
    "\n",
    "# Initialize a dictionary to store flow data from each file\n",
    "flow_data = {}\n",
    "filtered_lat = []\n",
    "filtered_long = []\n",
    "# Process each file\n",
    "for i, file in enumerate(csv_files):\n",
    "    # print(file.split(\"/\")[-1].split(\"_\")[0])\n",
    "    st = file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    try:\n",
    "        # print(i)\n",
    "        # Read CSV without headers and assign column names\n",
    "        df = pd.read_csv(file, header=None, names=columns)\n",
    "        # print(df)\n",
    "\n",
    "        # Create a date column\n",
    "        df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]])\n",
    "\n",
    "        # Filter data within the required date range\n",
    "        df_filtered = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)][[\"Date\", \"Flow Data\"]]\n",
    "        # print(df_filtered.empty)\n",
    "\n",
    "        # If data is available, store it in the dictionary with a unique name\n",
    "        if not df_filtered.empty:\n",
    "            flow_data[f\"Flow_{i+1}\"] = df_filtered.set_index(\"Date\")[\"Flow Data\"]\n",
    "            filtered_id = np.where(STAID==st)\n",
    "            # print(filtered_id)\n",
    "            filtered_lat.append(lats[filtered_id[0]].values[0])\n",
    "            filtered_long.append(longs[filtered_id[0]].values[0])\n",
    "            \n",
    "        else:\n",
    "            print('exclude',file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Merge all data side by side\n",
    "if flow_data:\n",
    "    merged_df = pd.concat(flow_data.values(), axis=1)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    output_file = f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.hyg\"\n",
    "    locations = pd.DataFrame({\n",
    "    '%X-Location': filtered_long,\n",
    "    'Y-Location': filtered_lat\n",
    "    })\n",
    "    locations.to_csv(f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.txt\", index=False) #%X-Location,Y-Location\n",
    "    # merged_df.to_csv(output_file, index=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Write the two custom lines to the top of the file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Sonuma Floods {event}\\n\")  # First custom line\n",
    "        f.write(\"%Time(hr) Discharge(cms)\\n\")  # Second custom line\n",
    "        \n",
    "    # Append the DataFrame to the file\n",
    "    merged_df.insert(0, 'Hours', hours_since_start)\n",
    "    merged_df.to_csv(output_file,mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Merged file saved as '{output_file}'.\")\n",
    "else:\n",
    "    print(\"No matching data found for the given date range.\")\n",
    "# print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as '/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_event3.hyg'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing CSV files\n",
    "event = \"event3\"\n",
    "# Define the date range\n",
    "start_date = pd.Timestamp(\"2017-01-09\")\n",
    "end_date = pd.Timestamp(\"2017-02-04\")\n",
    "\n",
    "\n",
    "# Generate a date range with a 24-hour frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='24h')\n",
    "\n",
    "# Calculate the number of hours since the start date and convert to integers\n",
    "hours_since_start = ((date_range - start_date).total_seconds() / 3600).astype(int)  # Convert seconds to hours and then to integers\n",
    "\n",
    "\n",
    "csv_dir = f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/Obs_Flow/{event}\"  # Change this to your CSV folder\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))  # Find all CSV files\n",
    "obs_location= f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/sonoma_USGS_Obs.csv\"\n",
    "df_location = pd.read_csv(obs_location)\n",
    "lats = df_location[\"LAT_GAGE\"]\n",
    "longs = df_location[\"LNG_GAGE\"]\n",
    "STAID = df_location[\"STAID\"].values.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Define column names (since there are no headers in files)\n",
    "columns = [\"Gauge ID\", \"Year\", \"Month\", \"Day\", \"Flow Data\"]\n",
    "\n",
    "# Initialize a dictionary to store flow data from each file\n",
    "flow_data = {}\n",
    "filtered_lat = []\n",
    "filtered_long = []\n",
    "# Process each file\n",
    "for i, file in enumerate(csv_files):\n",
    "    # print(file.split(\"/\")[-1].split(\"_\")[0])\n",
    "    st = file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    try:\n",
    "        # print(i)\n",
    "        # Read CSV without headers and assign column names\n",
    "        df = pd.read_csv(file, header=None, names=columns)\n",
    "        # print(df)\n",
    "\n",
    "        # Create a date column\n",
    "        df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]])\n",
    "\n",
    "        # Filter data within the required date range\n",
    "        df_filtered = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)][[\"Date\", \"Flow Data\"]]\n",
    "        # print(df_filtered.empty)\n",
    "\n",
    "        # If data is available, store it in the dictionary with a unique name\n",
    "        if not df_filtered.empty:\n",
    "            flow_data[f\"Flow_{i+1}\"] = df_filtered.set_index(\"Date\")[\"Flow Data\"]\n",
    "            filtered_id = np.where(STAID==st)\n",
    "            # print(filtered_id)\n",
    "            filtered_lat.append(lats[filtered_id[0]].values[0])\n",
    "            filtered_long.append(longs[filtered_id[0]].values[0])\n",
    "            \n",
    "        else:\n",
    "            print('exclude',file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Merge all data side by side\n",
    "if flow_data:\n",
    "    merged_df = pd.concat(flow_data.values(), axis=1)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    output_file = f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.hyg\"\n",
    "    locations = pd.DataFrame({\n",
    "    '%X-Location': filtered_long,\n",
    "    'Y-Location': filtered_lat\n",
    "    })\n",
    "    locations.to_csv(f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.txt\", index=False) #%X-Location,Y-Location\n",
    "    # merged_df.to_csv(output_file, index=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Write the two custom lines to the top of the file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Sonuma Floods {event}\\n\")  # First custom line\n",
    "        f.write(\"%Time(hr) Discharge(cms)\\n\")  # Second custom line\n",
    "        \n",
    "    # Append the DataFrame to the file\n",
    "    merged_df.insert(0, 'Hours', hours_since_start)\n",
    "    merged_df.to_csv(output_file,mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Merged file saved as '{output_file}'.\")\n",
    "else:\n",
    "    print(\"No matching data found for the given date range.\")\n",
    "# print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as '/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_event4.hyg'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing CSV files\n",
    "event = \"event4\"\n",
    "# Define the date range\n",
    "start_date = pd.Timestamp(\"2014-12-10\")\n",
    "end_date = pd.Timestamp(\"2014-12-12\")\n",
    "\n",
    "\n",
    "# Generate a date range with a 24-hour frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='24h')\n",
    "\n",
    "# Calculate the number of hours since the start date and convert to integers\n",
    "hours_since_start = ((date_range - start_date).total_seconds() / 3600).astype(int)  # Convert seconds to hours and then to integers\n",
    "\n",
    "\n",
    "csv_dir = f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/Obs_Flow/{event}\"  # Change this to your CSV folder\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))  # Find all CSV files\n",
    "obs_location= f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/sonoma_USGS_Obs.csv\"\n",
    "df_location = pd.read_csv(obs_location)\n",
    "lats = df_location[\"LAT_GAGE\"]\n",
    "longs = df_location[\"LNG_GAGE\"]\n",
    "STAID = df_location[\"STAID\"].values.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Define column names (since there are no headers in files)\n",
    "columns = [\"Gauge ID\", \"Year\", \"Month\", \"Day\", \"Flow Data\"]\n",
    "\n",
    "# Initialize a dictionary to store flow data from each file\n",
    "flow_data = {}\n",
    "filtered_lat = []\n",
    "filtered_long = []\n",
    "# Process each file\n",
    "for i, file in enumerate(csv_files):\n",
    "    # print(file.split(\"/\")[-1].split(\"_\")[0])\n",
    "    st = file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    try:\n",
    "        # print(i)\n",
    "        # Read CSV without headers and assign column names\n",
    "        df = pd.read_csv(file, header=None, names=columns)\n",
    "        # print(df)\n",
    "\n",
    "        # Create a date column\n",
    "        df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]])\n",
    "\n",
    "        # Filter data within the required date range\n",
    "        df_filtered = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)][[\"Date\", \"Flow Data\"]]\n",
    "        # print(df_filtered.empty)\n",
    "\n",
    "        # If data is available, store it in the dictionary with a unique name\n",
    "        if not df_filtered.empty:\n",
    "            flow_data[f\"Flow_{i+1}\"] = df_filtered.set_index(\"Date\")[\"Flow Data\"]\n",
    "            filtered_id = np.where(STAID==st)\n",
    "            # print(filtered_id)\n",
    "            filtered_lat.append(lats[filtered_id[0]].values[0])\n",
    "            filtered_long.append(longs[filtered_id[0]].values[0])\n",
    "            \n",
    "        else:\n",
    "            print('exclude',file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Merge all data side by side\n",
    "if flow_data:\n",
    "    merged_df = pd.concat(flow_data.values(), axis=1)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    output_file = f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.hyg\"\n",
    "    locations = pd.DataFrame({\n",
    "    '%X-Location': filtered_long,\n",
    "    'Y-Location': filtered_lat\n",
    "    })\n",
    "    locations.to_csv(f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.txt\", index=False) #%X-Location,Y-Location\n",
    "    # merged_df.to_csv(output_file, index=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Write the two custom lines to the top of the file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Sonuma Floods {event}\\n\")  # First custom line\n",
    "        f.write(\"%Time(hr) Discharge(cms)\\n\")  # Second custom line\n",
    "        \n",
    "    # Append the DataFrame to the file\n",
    "    merged_df.insert(0, 'Hours', hours_since_start)\n",
    "    merged_df.to_csv(output_file,mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Merged file saved as '{output_file}'.\")\n",
    "else:\n",
    "    print(\"No matching data found for the given date range.\")\n",
    "# print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as '/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_event5.hyg'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing CSV files\n",
    "event = \"event5\"\n",
    "# Define the date range\n",
    "start_date = pd.Timestamp(\"2006-03-29\")\n",
    "end_date = pd.Timestamp(\"2006-04-06\")\n",
    "\n",
    "\n",
    "# Generate a date range with a 24-hour frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='24h')\n",
    "\n",
    "# Calculate the number of hours since the start date and convert to integers\n",
    "hours_since_start = ((date_range - start_date).total_seconds() / 3600).astype(int)  # Convert seconds to hours and then to integers\n",
    "\n",
    "\n",
    "csv_dir = f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/Obs_Flow/{event}\"  # Change this to your CSV folder\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))  # Find all CSV files\n",
    "obs_location= f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/sonoma_USGS_Obs.csv\"\n",
    "df_location = pd.read_csv(obs_location)\n",
    "lats = df_location[\"LAT_GAGE\"]\n",
    "longs = df_location[\"LNG_GAGE\"]\n",
    "STAID = df_location[\"STAID\"].values.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Define column names (since there are no headers in files)\n",
    "columns = [\"Gauge ID\", \"Year\", \"Month\", \"Day\", \"Flow Data\"]\n",
    "\n",
    "# Initialize a dictionary to store flow data from each file\n",
    "flow_data = {}\n",
    "filtered_lat = []\n",
    "filtered_long = []\n",
    "# Process each file\n",
    "for i, file in enumerate(csv_files):\n",
    "    # print(file.split(\"/\")[-1].split(\"_\")[0])\n",
    "    st = file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    try:\n",
    "        # print(i)\n",
    "        # Read CSV without headers and assign column names\n",
    "        df = pd.read_csv(file, header=None, names=columns)\n",
    "        # print(df)\n",
    "\n",
    "        # Create a date column\n",
    "        df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]])\n",
    "\n",
    "        # Filter data within the required date range\n",
    "        df_filtered = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)][[\"Date\", \"Flow Data\"]]\n",
    "        # print(df_filtered.empty)\n",
    "\n",
    "        # If data is available, store it in the dictionary with a unique name\n",
    "        if not df_filtered.empty:\n",
    "            flow_data[f\"Flow_{i+1}\"] = df_filtered.set_index(\"Date\")[\"Flow Data\"]\n",
    "            filtered_id = np.where(STAID==st)\n",
    "            # print(filtered_id)\n",
    "            filtered_lat.append(lats[filtered_id[0]].values[0])\n",
    "            filtered_long.append(longs[filtered_id[0]].values[0])\n",
    "            \n",
    "        else:\n",
    "            print('exclude',file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Merge all data side by side\n",
    "if flow_data:\n",
    "    merged_df = pd.concat(flow_data.values(), axis=1)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    output_file = f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.hyg\"\n",
    "    locations = pd.DataFrame({\n",
    "    '%X-Location': filtered_long,\n",
    "    'Y-Location': filtered_lat\n",
    "    })\n",
    "    locations.to_csv(f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.txt\", index=False) #%X-Location,Y-Location\n",
    "    # merged_df.to_csv(output_file, index=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Write the two custom lines to the top of the file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Sonuma Floods {event}\\n\")  # First custom line\n",
    "        f.write(\"%Time(hr) Discharge(cms)\\n\")  # Second custom line\n",
    "        \n",
    "    # Append the DataFrame to the file\n",
    "    merged_df.insert(0, 'Hours', hours_since_start)\n",
    "    merged_df.to_csv(output_file,mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Merged file saved as '{output_file}'.\")\n",
    "else:\n",
    "    print(\"No matching data found for the given date range.\")\n",
    "# print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as '/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_event6.hyg'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing CSV files\n",
    "event = \"event6\"\n",
    "# Define the date range\n",
    "start_date = pd.Timestamp(\"2005-12-30\")\n",
    "end_date = pd.Timestamp(\"2006-01-04\")\n",
    "\n",
    "\n",
    "# Generate a date range with a 24-hour frequency\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='24h')\n",
    "\n",
    "# Calculate the number of hours since the start date and convert to integers\n",
    "hours_since_start = ((date_range - start_date).total_seconds() / 3600).astype(int)  # Convert seconds to hours and then to integers\n",
    "\n",
    "\n",
    "csv_dir = f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/Obs_Flow/{event}\"  # Change this to your CSV folder\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))  # Find all CSV files\n",
    "obs_location= f\"/usr/workspace/lazin1/anaconda_dane/envs/RAPID/Codes/Sonoma_shapefile/sonoma_USGS_Obs.csv\"\n",
    "df_location = pd.read_csv(obs_location)\n",
    "lats = df_location[\"LAT_GAGE\"]\n",
    "longs = df_location[\"LNG_GAGE\"]\n",
    "STAID = df_location[\"STAID\"].values.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Define column names (since there are no headers in files)\n",
    "columns = [\"Gauge ID\", \"Year\", \"Month\", \"Day\", \"Flow Data\"]\n",
    "\n",
    "# Initialize a dictionary to store flow data from each file\n",
    "flow_data = {}\n",
    "filtered_lat = []\n",
    "filtered_long = []\n",
    "# Process each file\n",
    "for i, file in enumerate(csv_files):\n",
    "    # print(file.split(\"/\")[-1].split(\"_\")[0])\n",
    "    st = file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    try:\n",
    "        # print(i)\n",
    "        # Read CSV without headers and assign column names\n",
    "        df = pd.read_csv(file, header=None, names=columns)\n",
    "        # print(df)\n",
    "\n",
    "        # Create a date column\n",
    "        df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]])\n",
    "\n",
    "        # Filter data within the required date range\n",
    "        df_filtered = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)][[\"Date\", \"Flow Data\"]]\n",
    "        # print(df_filtered.empty)\n",
    "\n",
    "        # If data is available, store it in the dictionary with a unique name\n",
    "        if not df_filtered.empty:\n",
    "            flow_data[f\"Flow_{i+1}\"] = df_filtered.set_index(\"Date\")[\"Flow Data\"]\n",
    "            filtered_id = np.where(STAID==st)\n",
    "            # print(filtered_id)\n",
    "            filtered_lat.append(lats[filtered_id[0]].values[0])\n",
    "            filtered_long.append(longs[filtered_id[0]].values[0])\n",
    "            \n",
    "        else:\n",
    "            print('exclude',file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Merge all data side by side\n",
    "if flow_data:\n",
    "    merged_df = pd.concat(flow_data.values(), axis=1)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    output_file = f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.hyg\"\n",
    "    locations = pd.DataFrame({\n",
    "    '%X-Location': filtered_long,\n",
    "    'Y-Location': filtered_lat\n",
    "    })\n",
    "    locations.to_csv(f\"/usr/workspace/lazin1/Codes/triton/input/strmflow/case_sonoma_{event}.txt\", index=False) #%X-Location,Y-Location\n",
    "    # merged_df.to_csv(output_file, index=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Write the two custom lines to the top of the file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Sonuma Floods {event}\\n\")  # First custom line\n",
    "        f.write(\"%Time(hr) Discharge(cms)\\n\")  # Second custom line\n",
    "        \n",
    "    # Append the DataFrame to the file\n",
    "    merged_df.insert(0, 'Hours', hours_since_start)\n",
    "    merged_df.to_csv(output_file,mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Merged file saved as '{output_file}'.\")\n",
    "else:\n",
    "    print(\"No matching data found for the given date range.\")\n",
    "# print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
